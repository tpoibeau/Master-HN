{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master HN PSL ‚Äì TAL 09/02/2022\n",
    "## [Spacy](https://spacy.io)\n",
    "\n",
    "Notebook con√ßu par C. Plancq (2021), mise √† jour T. Poibeau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Biblioth√®que logicielle de TAL √©crite en Python (et Cython)\n",
    "- √âtiquetage POS, lemmatisation, analyse syntaxique, entit√©s nomm√©es, word embedding, transformers\n",
    "- Usage de mod√®les neuronaux\n",
    "- Int√©gration ais√©e de biblioth√®ques de deep learning\n",
    "- v3.0.3 ([github](https://github.com/explosion/spaCy))\n",
    "- Licence MIT (Open Source) pour le code\n",
    "    - Licences ouvertes diverses pour les mod√®les\n",
    "- Produit de la soci√©t√© [explosion.ai](https://explosion.ai/). Fond√© par :¬†Matthew Honnibal ([@honnibal](https://twitter.com/honnibal)) et Ines Montani ([@_inesmontani](https://twitter.com/_inesmontani))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi Spacy ?\n",
    "\n",
    "- C'est du Python üôå üéâ\n",
    "- Plut√¥t simple √† prendre en main\n",
    "- Tr√®s bien document√©, √† notre avis. D'ailleurs plut√¥t que ce notebook, suivez l'excellent tutorial d'Ines Montani : [https://course.spacy.io/](https://course.spacy.io/)\n",
    "- Couvre les traitements d'une cha√Æne de TAL typique\n",
    "- Pas mal utilis√© dans l'industrie\n",
    "- MAIS ce n'est pas forc√©ment l'outil qui donne les meilleurs r√©sultats pour le fran√ßais dans toutes les t√¢ches de TAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy et les autres\n",
    "\n",
    "Spacy est *un* des frameworks de TAL disponibles\n",
    "\n",
    "- [NLTK](http://www.nltk.org/) :¬†python, orient√© p√©dagogie, pas de mod√®les neuronaux inclus mais se combine bien avec TensorFlow, PyTorch ou AlleNLP\n",
    "- [Stanford Core¬†NLP](https://stanfordnlp.github.io/stanfordnlp/) :¬†java, mod√®les pour 53 langues (UD), r√©solution de la cor√©ference.\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/) :¬†python, nouveau framework de Stanford, mod√®les neuronaux entra√Æn√©s sur donn√©es UD <small>[https://github.com/explosion/spacy-stanza](https://github.com/explosion/spacy-stanza) permet d'utiliser les mod√®les de Stanford avec Spacy</small>\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "- [DKPro](https://dkpro.github.io/)\n",
    "- [flair](https://github.com/zalandoresearch/flair) : le framework de Zalando, tr√®s bonnes performances en reconnaissance d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation\n",
    "\n",
    "dans un terminal\n",
    "```bash\n",
    "python3 -m pip install -U --user spacy \n",
    "#ou pip install -U --user spacy\n",
    "```\n",
    "- installation du mod√®le fran√ßais\n",
    "```bash\n",
    "python3 -m spacy download fr_core_news_md\n",
    "#ou python3 -m spacy download fr_core_news_sm \n",
    "```\n",
    "- v√©rification\n",
    "```bash\n",
    "python3 -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mod√®les\n",
    "\n",
    "- Spacy utilise des mod√®les statistiques qui permettent de pr√©dire des annotations linguistiques\n",
    "- 16 langues :¬†allemand, anglais, chinois, danois, espagnol, fran√ßais, italien, japonais, lituanien, n√©erlandais, grec, norv√©gien, polonais, portugais, roumain, russe + mod√®le multi langues\n",
    "- 4 mod√®les pour le fran√ßais\n",
    "    - fr_core_news_sm (tagger, morphologizer, lemmatizer, parser, ner) 16 Mo\n",
    "    - fr_core_news_md (tagger, morphologizer, lemmatizer, parser, ner, vectors) 45 Mo\n",
    "    - fr_core_news_lg (tagger, morphologizer, lemmatizer, parser, ner, vectors) 546 Mo\n",
    "    - fr_dep_news_trf (tagger, morphologizer, lemmatizer, parser) 381 Mo\n",
    "- mod√®les `fr` appris sur les corpus [Sequoia](https://deep-sequoia.inria.fr/fr/) et [WikiNer](https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500) sauf le mod√®le `trf` qui est issu de camembert-base distribu√© par [Hugging Face](https://huggingface.co/camembert-base).\n",
    "- Tous ces mod√®les, quelque soient leur type ou leur langue, s'utilisent de la m√™me fa√ßon, avec la m√™me API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    "- *si vous voulez utiliser Spacy prenez le temps de lire la [documentation](https://spacy.io/usage), ici ce ne sera qu'un coup d'≈ìil incomplet*\n",
    "- un mod√®le est une instance de la classe `Language`, il est adapt√© √† une langue en particulier\n",
    "- un mod√®le incorpore un vocabulaire, des poids, des vecteurs de mots, une configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.fr.French"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- le traitement fonctionne avec un [*pipeline*](https://spacy.io/usage/spacy-101#pipelines) pour convertir un texte en objet `Doc` (texte annot√©)\n",
    "- par d√©faut `tokenizer` > `tagger` > `parser` > `ner` > `‚Ä¶`\n",
    "- depuis la v3 le pipeline devient `tok2vec` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`  \n",
    "  ou `transformer` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`\n",
    "- l'utilisateur peut ajouter des √©tapes ou en retrancher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x12739e160>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x284c69e20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x285971b40>),\n",
       " ('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer at 0x284cc1740>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_md', disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour au pipeline par d√©faut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x137e5fee0>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x137e5fb20>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x126e02cf0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x284c6a8c0>),\n",
       " ('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer at 0x137ee8d80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x126e02c10>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Un objet `Doc` est une s√©quence d'objets `Token` (voir l'[API](https://spacy.io/api/token))\n",
    " - Le texte d'origine est d√©coup√© en phrases, tokeniz√©, annot√© en POS, lemme, syntaxe (d√©pendance) et en entit√©s nomm√©es (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"L‚ÄôOrganisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì tokenization\n",
    "\n",
    "La tokenization de Spacy est non-destructive. Vous pouvez d√©couper un texte en tokens et le restituer dans sa forme originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'\n",
      "Organisation\n",
      "des\n",
      "Nations\n",
      "unies\n",
      "(\n",
      "ONU\n",
      ")\n",
      "a\n",
      "lanc√©\n",
      "mardi\n",
      "un\n",
      "appel\n",
      "d'\n",
      "urgence\n",
      "pour\n",
      "lever\n",
      "des\n",
      "dizaines\n",
      "de\n",
      "millions\n",
      "de\n",
      "dollars\n",
      "afin\n",
      "de\n",
      "prot√©ger\n",
      "les\n",
      "r√©fugi√©s\n",
      "vuln√©rables\n",
      "face\n",
      "√†\n",
      "la\n",
      "propagation\n",
      "du\n",
      "nouveau\n",
      "coronavirus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus."
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text_with_ws, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì √©tiquetage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les annotations portant sur les tokens sont accessibles via les attributs des objets de type `token`‚ÄØ: [https://spacy.io/api/token#attributes](https://spacy.io/api/token#attributes)  \n",
    "  - `pos_` contient l'√©tiquette de partie du discours de [universal dependancies](https://universaldependencies.org/docs/u/pos/)\n",
    "  - `tag_` contient l'√©tiquette du corpus original, parfois plus d√©taill√©e\n",
    "  - `lemma_` pour le lemme\n",
    "  - `morph` pour l'analyse morphologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L' DET Definite=Def|Number=Sing|PronType=Art le\n",
      "Organisation NOUN Gender=Fem|Number=Sing organisation\n",
      "des ADP Definite=Def|Number=Plur|PronType=Art de\n",
      "Nations PROPN  Nations\n",
      "unies ADJ Gender=Fem|Number=Plur uni\n",
      "( PUNCT  (\n",
      "ONU PROPN Gender=Fem|Number=Sing ONU\n",
      ") PUNCT  )\n",
      "a AUX Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin avoir\n",
      "lanc√© VERB Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part lancer\n",
      "mardi NOUN Gender=Masc|Number=Sing mardi\n",
      "un DET Definite=Ind|Gender=Masc|Number=Sing|PronType=Art un\n",
      "appel NOUN Gender=Masc|Number=Sing appel\n",
      "d' ADP  de\n",
      "urgence NOUN Gender=Fem|Number=Sing urgence\n",
      "pour ADP  pour\n",
      "lever VERB VerbForm=Inf lever\n",
      "des DET Definite=Ind|Number=Plur|PronType=Art un\n",
      "dizaines NOUN Gender=Fem|Number=Plur dizaine\n",
      "de ADP  de\n",
      "millions NOUN Gender=Masc|NumType=Card|Number=Plur million\n",
      "de ADP  de\n",
      "dollars NOUN Gender=Masc|Number=Plur dollar\n",
      "afin ADV  afin\n",
      "de ADP  de\n",
      "prot√©ger VERB VerbForm=Inf prot√©ger\n",
      "les DET Definite=Def|Number=Plur|PronType=Art le\n",
      "r√©fugi√©s NOUN Gender=Masc|Number=Plur r√©fugi√©\n",
      "vuln√©rables ADJ Number=Plur vuln√©rable\n",
      "face NOUN Gender=Fem|Number=Sing face\n",
      "√† ADP  √†\n",
      "la DET Definite=Def|Gender=Fem|Number=Sing|PronType=Art le\n",
      "propagation NOUN Gender=Fem|Number=Sing propagation\n",
      "du ADP Definite=Def|Gender=Masc|Number=Sing|PronType=Art de\n",
      "nouveau ADJ Gender=Masc|Number=Sing nouveau\n",
      "coronavirus NOUN Gender=Masc|Number=Sing coronavirus\n",
      ". PUNCT  .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.morph, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traiter plusieurs textes en s√©rie, il est recommand√© d'utiliser [nlp.pipe](https://spacy.io/api/language#pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Cadine avait un tr√®s-mauvais caract√®re. Elle ne s‚Äôaccommodait pas du r√¥le de servante.\",\n",
    "    \"Aussi finit-elle par s‚Äô√©tablir pour son compte.\",\n",
    "    \"Comme elle √©tait alors √¢g√©e de treize ans, et qu‚Äôelle ne pouvait r√™ver le grand commerce, un banc de vente de l‚Äôall√©e aux fleurs, elle vendit des bouquets de violettes d‚Äôun sou, piqu√©s dans un lit de mousse, sur un √©ventaire d‚Äôosier pendu √† son cou.\",\n",
    "    \"Elle r√¥dait toute la journ√©e dans les Halles, autour des Halles, promenant son bout de pelouse.\",\n",
    "    \"C‚Äô√©tait l√† sa joie, cette fl√¢nerie continuelle, qui lui d√©gourdissait les jambes, qui la tirait des longues heures pass√©es √† faire des bouquets, les genoux pli√©s, sur une chaise basse.\",\n",
    "    \"Maintenant, elle tournait ses violettes en marchant, elle les tournait comme des fuseaux, avec une merveilleuse l√©g√®ret√© de doigts ; elle comptait six √† huit fleurs, selon la saison, pliait en deux un brin de jonc, ajoutait une feuille, roulait un fil mouill√© ; et, entre ses dents de jeune loup, elle cassait le fil.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "1. Extrayez de la s√©rie de phrases ci-dessus la liste des noms communs\n",
    "2. Comptez le nombre de tokens au masculin et au f√©minin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si NER (*Named Entity Recognition*) fait partie de votre mod√®le, vos donn√©es seront annot√©es √©galement en entit√©s nomm√©es.  \n",
    "Vous pouvez y acc√©der avec l'attribut `ent_type_` des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L' \n",
      "Organisation ORG\n",
      "des ORG\n",
      "Nations ORG\n",
      "unies ORG\n",
      "( \n",
      "ONU ORG\n",
      ") \n",
      "a \n",
      "lanc√© \n",
      "mardi \n",
      "un \n",
      "appel \n",
      "d‚Äô \n",
      "urgence \n",
      "pour \n",
      "lever \n",
      "des \n",
      "dizaines \n",
      "de \n",
      "millions \n",
      "de \n",
      "dollars \n",
      "afin \n",
      "de \n",
      "prot√©ger \n",
      "les \n",
      "r√©fugi√©s \n",
      "vuln√©rables \n",
      "face \n",
      "√† \n",
      "la \n",
      "propagation \n",
      "du \n",
      "nouveau \n",
      "coronavirus \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou acc√©der directement aux entit√©s de l'objet `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organisation des Nations unies ORG\n",
      "ONU ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy int√®gre un outil de visualisation pour l'annotation en entit√©s nomm√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">L'\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Organisation des Nations unies\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ONU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Le pr√©sident \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Xi Jinping\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " a affirm√© que la propagation du coronavirus √©tait ¬´‚ÄØpratiquement jugul√©e‚ÄØ¬ª. Il s‚Äôest d‚Äôailleurs rendu pour la premi√®re fois √† \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wuhan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", la capitale de la province du \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hubei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", le berceau du Covid-19.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('Le pr√©sident Xi Jinping a affirm√© que la propagation du coronavirus √©tait ¬´‚ÄØpratiquement jugul√©e‚ÄØ¬ª. Il s‚Äôest d‚Äôailleurs rendu pour la premi√®re fois √† Wuhan, la capitale de la province du Hubei, le berceau du Covid-19.')\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Derri√®re lui, sur le carreau de la \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rue Rambuteau\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", on vendait des fruits.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "\n",
    "Dans `data/Le_Ventre_de_Paris-short.txt` (ou un texte de votre choix), comptez la fr√©quence de chaque entit√© de type PER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì analyse syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse syntaxique ou *parsing* de Spacy est une analyse en d√©pendance. La plupart sinon la totalit√© des mod√®les utilis√©s viennent de https://universaldependencies.org\n",
    "\n",
    "Dans l'analyse en d√©pendance produite par Spacy, chaque mot d'une phrase a un gouverneur unique (*head*), la relation de d√©pendance entre le mot et son gouverneur est typ√©e (*nsubj*, *obj*, ‚Ä¶).  \n",
    "Pour la t√™te de la phrase on utilise la relation *ROOT*.\n",
    "\n",
    "La structure produite par l'analyse syntaxique est un arbre, un graphe acyclique et connexe. Les tokens sont les n≈ìuds, les arcs sont les d√©pendances, le type de la relation est l'√©tiquette de l'arc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`displacy` fournit un outil de visualisation bien pratique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"a7187e329f8845b5b66122fb13d15bb5-0\" class=\"displacy\" width=\"1220\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Derri√®re</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">lui,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">sur</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">le</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">carreau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">de</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">la</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">rue</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">Rambuteau,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">vendait</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">des</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">fruits.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-0\" stroke-width=\"2px\" d=\"M70,227.0 C70,182.0 120.0,182.0 120.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,229.0 L62,217.0 78,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-1\" stroke-width=\"2px\" d=\"M160,227.0 C160,2.0 950.0,2.0 950.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:mod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,229.0 L152,217.0 168,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-2\" stroke-width=\"2px\" d=\"M250,227.0 C250,137.0 395.0,137.0 395.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,229.0 L242,217.0 258,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-3\" stroke-width=\"2px\" d=\"M340,227.0 C340,182.0 390.0,182.0 390.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,229.0 L332,217.0 348,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-4\" stroke-width=\"2px\" d=\"M430,227.0 C430,47.0 945.0,47.0 945.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:mod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,229.0 L422,217.0 438,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-5\" stroke-width=\"2px\" d=\"M520,227.0 C520,137.0 665.0,137.0 665.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,229.0 L512,217.0 528,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-6\" stroke-width=\"2px\" d=\"M610,227.0 C610,182.0 660.0,182.0 660.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610,229.0 L602,217.0 618,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-7\" stroke-width=\"2px\" d=\"M430,227.0 C430,92.0 670.0,92.0 670.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670.0,229.0 L678.0,217.0 662.0,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-8\" stroke-width=\"2px\" d=\"M700,227.0 C700,182.0 750.0,182.0 750.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,229.0 L758.0,217.0 742.0,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-9\" stroke-width=\"2px\" d=\"M880,227.0 C880,182.0 930.0,182.0 930.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,229.0 L872,217.0 888,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-10\" stroke-width=\"2px\" d=\"M1060,227.0 C1060,182.0 1110.0,182.0 1110.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060,229.0 L1052,217.0 1068,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7187e329f8845b5b66122fb13d15bb5-0-11\" stroke-width=\"2px\" d=\"M970,227.0 C970,137.0 1115.0,137.0 1115.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7187e329f8845b5b66122fb13d15bb5-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1115.0,229.0 L1123.0,217.0 1107.0,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe √©galement un outil issu d'un d√©veloppement ind√©pendant :¬†[explacy](https://spacy.io/universe/project/explacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree     Token     Dep type Lemma     Part of Sp\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "         ‚îå‚îÄ‚ñ∫ Derri√®re  case     derri√®re  ADP       \n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îî‚îÄ‚îÄ lui       obl:mod  lui       PRON      \n",
      "‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,         punct    ,         PUNCT     \n",
      "‚îÇ‚îÇ      ‚îå‚îÄ‚îÄ‚ñ∫ sur       case     sur       ADP       \n",
      "‚îÇ‚îÇ      ‚îÇ‚îå‚îÄ‚ñ∫ le        det      le        DET       \n",
      "‚îÇ‚îÇ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚î¥‚î¥‚îÄ‚îÄ carreau   obl:mod  carreau   NOUN      \n",
      "‚îÇ‚îÇ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ de        case     de        ADP       \n",
      "‚îÇ‚îÇ‚îÇ  ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ la        det      le        DET       \n",
      "‚îÇ‚îÇ‚îÇ  ‚îî‚îÄ‚ñ∫‚îî‚îº‚îÄ‚îÄ rue       nmod     rue       NOUN      \n",
      "‚îÇ‚îÇ‚îÇ      ‚îî‚îÄ‚ñ∫ Rambuteau nmod     Rambuteau PROPN     \n",
      "‚îÇ‚îÇ‚îÇ     ‚îå‚îÄ‚îÄ‚ñ∫ ,         punct    ,         PUNCT     \n",
      "‚îÇ‚îÇ‚îÇ     ‚îÇ‚îå‚îÄ‚ñ∫ on        nsubj    on        PRON      \n",
      "‚îî‚î¥‚î¥‚îÄ‚îÄ‚î¨‚î¨‚îÄ‚î¥‚î¥‚îÄ‚îÄ vendait   ROOT     vendre    VERB      \n",
      "     ‚îÇ‚îÇ  ‚îå‚îÄ‚ñ∫ des       det      un        DET       \n",
      "     ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ fruits    obj      fruit     NOUN      \n",
      "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .         punct    .         PUNCT     \n"
     ]
    }
   ],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp, 'Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi r√©cup√©rer parcourir les tokens et afficher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derri√®re CASE lui\n",
      "lui OBL:MOD vendait\n",
      ", PUNCT vendait\n",
      "sur CASE carreau\n",
      "le DET carreau\n",
      "carreau OBL:MOD vendait\n",
      "de CASE rue\n",
      "la DET rue\n",
      "rue NMOD carreau\n",
      "Rambuteau NMOD rue\n",
      ", PUNCT vendait\n",
      "on NSUBJ vendait\n",
      "vendait ROOT vendait\n",
      "des DET fruits\n",
      "fruits OBJ vendait\n",
      ". PUNCT vendait\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_.upper(), token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les attributs de token suivant peuvent √™tre utilis√©s pour parcourir l'arbre de d√©pendance :¬†\n",
    "- `children` les tokens d√©pendants du token\n",
    "- `subtree` tous les descendants du token\n",
    "- `ancestors` tous les parents du token\n",
    "- `rights` les enfants √† droite du token\n",
    "- `lefts` les enfants √† gauche du token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire de la phrase pr√©c√©dente le triplet sujet-verbe-objet comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(on, vendait, fruits)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = [token for token in doc if token.head == token][0]\n",
    "subjects = [tok for tok in root.lefts if tok.dep_ == \"nsubj\"]\n",
    "subject = subjects[0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ == \"obj\"]\n",
    "obj = objs[0]\n",
    "subject, root, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "des\n",
      "fruits\n"
     ]
    }
   ],
   "source": [
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous\n",
    "\n",
    "1. Trouver et afficher l'objet de la phrase :¬†¬´ Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle. ¬ª\n",
    "\n",
    "2. Dans la liste `texts` vue plus haut, retrouvez les verbes dont Cadine ou le pronom 'elle' est sujet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching par r√®gle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy a une classe `Matcher` qui permet de rep√©rer des tokens ou des suites de tokens √† l'aide de patrons (*pattern*). Ces patrons peuvent porter sur la forme des tokens ou leurs attributs (pos, ent).  \n",
    "On peut aussi utiliser des cat√©gories comme `IS_ALPHA` ou `IS_NUM`, voir la [doc](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 8 en taille M\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "# en taille + lettres en maj\n",
    "matcher.add(\"tailles\", [pattern])\n",
    "\n",
    "doc = nlp(\"Ce mod√®le est aussi disponible en taille M ; je vous le conseille.\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√áa fonctionne pour les s√©quences comme ¬´ en taille M ¬ª ou ¬´ en taille XL ¬ª mais pas pour ¬´ vous l'avez en XL ? ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut essayer d'am√©liorer les r√®gles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en XL\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou encore :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9364735015326875510 tailles 3 5 en XL\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "sizes = ['XS', 'S', 'M', 'L', 'XL']\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Dans `data/Le_Ventre_de_Paris-short.txt`, trouver les s√©quences pronom - le lemme 'vendre'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependancy Matcher :¬†extraction de patrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis la v3, Spacy a ajout√© un *Dependancy Matcher* qui permet de faire de l'extraction de patrons syntaxiques. Il est maintenant possible de faire porter des requ√™tes sur l'arbre syntaxique et non plus seulement sur la s√©quence des tokens.  \n",
    "Ce dispositif utilise [Semgrex](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html), la syntaxe utilis√©e dans Tgrep et Tregex, les outils de requ√™te sur Treebank de Stanford.\n",
    "\n",
    "Voir la [documentation](https://spacy.io/usage/rule-based-matching#dependencymatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventre_short = \"\"\n",
    "with open('data/Le_Ventre_de_Paris-short.txt') as input_f:\n",
    "    ventre_short = input_f.read()\n",
    "doc = nlp(ventre_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vend\n",
      "vendant\n",
      "vendait\n",
      "vendait\n",
      "vendaient\n",
      "vendaient\n",
      "vend\n",
      "vendu\n",
      "vendu\n",
      "vendre\n",
      "vendait\n",
      "vendu\n",
      "vendais\n",
      "vendu\n",
      "vendrait\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"vendre\",    \n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"vendre\"}\n",
    "  }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    for t_id in t_ids:\n",
    "        print(doc[t_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbe, sujet, objet :¬† acheta -> il -> derniers\n",
      "objet complet :¬† ses deux derniers sous de pain\n",
      "Phrase compl√©te :¬† Mais, √† Vernon, il acheta ses deux derniers sous de pain.\n",
      "\n",
      "verbe, sujet, objet :¬† achetait -> elle -> qu‚Äô\n",
      "objet complet :¬† qu‚Äô\n",
      "Phrase compl√©te :¬† J‚Äô√©tais gamine, qu‚Äôelle achetait d√©j√† ses navets √† mon p√®re.\n",
      "\n",
      "verbe, sujet, objet :¬† achetait -> elle -> navets\n",
      "objet complet :¬† ses navets √† mon p√®re\n",
      "Phrase compl√©te :¬† J‚Äô√©tais gamine, qu‚Äôelle achetait d√©j√† ses navets √† mon p√®re.\n",
      "\n",
      "verbe, sujet, objet :¬† vendait -> on -> fruits\n",
      "objet complet :¬† des fruits\n",
      "Phrase compl√©te :¬† \n",
      "\n",
      "Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\n",
      "\n",
      "verbe, sujet, objet :¬† vendaient -> qui -> bottes\n",
      "objet complet :¬† des bottes de foug√®re et des paquets de feuilles de vigne , bien r√©guliers , attach√©s par quarterons\n",
      "Phrase compl√©te :¬† Ils s‚Äôarr√™t√®rent curieusement devant des femmes qui vendaient des bottes de foug√®re et des paquets de feuilles de vigne, bien r√©guliers, attach√©s par quarterons.\n",
      "\n",
      "verbe, sujet, objet :¬† vend -> Lui -> volaille\n",
      "objet complet :¬† toute la volaille qu‚Äô il veut\n",
      "Phrase compl√©te :¬† Lui, vend toute la volaille qu‚Äôil veut‚Ä¶\n",
      "\n",
      "verbe, sujet, objet :¬† achetait -> il -> morceau\n",
      "objet complet :¬† un morceau de dinde ou un morceau d‚Äô oie de douze\n",
      "Phrase compl√©te :¬† Quand Florent rentrait trop tard pour faire cuire quelque bout de viande, il achetait en bas un morceau de dinde ou un morceau d‚Äôoie de douze sous.\n",
      "\n",
      "verbe, sujet, objet :¬† vendu -> Il -> mobilier\n",
      "objet complet :¬† le pauvre mobilier de la rue Royer\n",
      "Phrase compl√©te :¬† Il avait vendu le pauvre mobilier de la rue Royer-Collard, et en gardait l‚Äôargent, quarante et quelques francs, pour que ce farceur de Quenu, disait-il, ne le jet√¢t pas par les fen√™tres.\n",
      "\n",
      "verbe, sujet, objet :¬† vendait -> elle -> o√π\n",
      "objet complet :¬† o√π\n",
      "Phrase compl√©te :¬† Lorsqu‚Äôelle le vit s‚Äô√©tablir aux Halles, √† deux pas du pavillon o√π elle vendait du beurre, des fromages et des ≈ìufs, elle l‚Äôaccusa d‚Äôavoir ¬´ invent√© √ßa pour la taquiner et lui porter mauvaise chance.\n",
      "\n",
      "verbe, sujet, objet :¬† vendait -> elle -> beurre\n",
      "objet complet :¬† du beurre , des fromages et des ≈ìufs\n",
      "Phrase compl√©te :¬† Lorsqu‚Äôelle le vit s‚Äô√©tablir aux Halles, √† deux pas du pavillon o√π elle vendait du beurre, des fromages et des ≈ìufs, elle l‚Äôaccusa d‚Äôavoir ¬´ invent√© √ßa pour la taquiner et lui porter mauvaise chance.\n",
      "\n",
      "verbe, sujet, objet :¬† achet√© -> Je -> vous\n",
      "objet complet :¬† vous\n",
      "Phrase compl√©te :¬† Je vous en ai achet√© avant-hier, du boudin‚Ä¶\n",
      "\n",
      "verbe, sujet, objet :¬† vendu -> vous -> m‚Äô\n",
      "objet complet :¬† m‚Äô\n",
      "Phrase compl√©te :¬† Elle se courba, les poings sur son comptoir ; et, d‚Äôune voix un peu rauque :\n",
      "\n",
      "‚Äî Dites donc, la semaine derni√®re, quand vous m‚Äôavez vendu cette paire de soles, vous savez, est-ce que je suis all√©e vous dire qu‚Äôelles √©taient pourries devant le monde !\n",
      "\n",
      "verbe, sujet, objet :¬† vendu -> vous -> paire\n",
      "objet complet :¬† cette paire de soles\n",
      "Phrase compl√©te :¬† Elle se courba, les poings sur son comptoir ; et, d‚Äôune voix un peu rauque :\n",
      "\n",
      "‚Äî Dites donc, la semaine derni√®re, quand vous m‚Äôavez vendu cette paire de soles, vous savez, est-ce que je suis all√©e vous dire qu‚Äôelles √©taient pourries devant le monde !\n",
      "\n",
      "verbe, sujet, objet :¬† vendrait -> Il -> semelles\n",
      "objet complet :¬† des semelles de bottes\n",
      "Phrase compl√©te :¬† Il vendrait des semelles de bottes pour des paires de soles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"vendre\",    \n",
    "        \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"vendre\", \"acheter\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"sujet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},  \n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"objet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obj\", \"iobj\", \"obl\"]}},  \n",
    "    }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    print(\"verbe, sujet, objet :¬†\", \" -> \".join([doc[t_id].text for t_id in t_ids]))\n",
    "    print(\"objet complet :¬†\", \" \".join([t.text for t in doc[t_ids[2]].subtree]))\n",
    "    print(\"Phrase compl√©te :¬†\", doc[t_ids[0]].sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Ajouter une r√®gle au motif pour trouver aussi l'objet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter les traitements de Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. re-tokenisation\n",
    "\n",
    "- voir [https://spacy.io/usage/linguistic-features#retokenization](https://spacy.io/usage/linguistic-features#retokenization)\n",
    "\n",
    "Dans l'exemple qui suit ¬´ quer-cra ¬ª sera tokeniz√© √† tort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pour', 'ADP', 'pour'), ('les', 'DET', 'le'), ('bons', 'ADJ', 'bon'), ('bails', 'NOUN', 'bail'), ('√ßa', 'PRON', 'cela'), ('va', 'VERB', 'aller'), ('grave', 'ADJ', 'grave'), ('quer', 'VERB', 'quer'), ('-', 'PUNCT', '-'), ('cra', 'PROPN', 'cra')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_)for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[7:], attrs={\"LEMMA\": \"quer-cra\", \"POS\": \"NOUN\"})\n",
    "print([(tok.text, tok.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ici c‚Äôest l‚Äôobjet doc qui est modifi√©, le r√©sultat mais pas le traitement. Nous allons voir comment faire pour modifier le traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modification de la tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pour', 'ADP', 'pour'), ('les', 'DET', 'le'), ('bons', 'ADJ', 'bon'), ('bails', 'NOUN', 'bail'), ('√ßa', 'PRON', 'cela'), ('va', 'VERB', 'aller'), ('grave', 'ADJ', 'grave'), ('quer-cra', 'PROPN', 'quer-cra')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "special_case = [{ORTH: \"quer-cra\"}]\n",
    "nlp.tokenizer.add_special_case(\"quer-cra\", special_case)\n",
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien modifi√© la tokenisation dans le mod√®le `nlp`. Cela n'affecte pas par contre l'√©tiquetage en POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entit√©s nomm√©es :¬†traitement par r√®gles\n",
    " - Voir [https://spacy.io/usage/rule-based-matching#entityruler](https://spacy.io/usage/rule-based-matching#entityruler)\n",
    " \n",
    "Spacy offre aussi un m√©canisme de traitement par r√®gle pour les entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant :  [('Chrome', 'MISC'), ('Criteo', 'MISC'), ('LiveRamp', 'MISC'), ('Index Exchange', 'MISC')]\n",
      "Apr√®s :  [('machin', 'ORG'), ('Chrome', 'ORG'), ('Criteo', 'ORG'), ('LiveRamp', 'ORG'), ('Index Exchange', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Avant : \", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={'overwrite_ents':True})\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Chrome\"},\n",
    "            {\"label\":\"ORG\", \"pattern\":\"machin\"},\n",
    "    {\"label\":\"ORG\", \"pattern\":\"Criteo\"},\n",
    "    {\"label\":\"ORG\",\"pattern\":\"LiveRamp\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Apr√®s : \", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
