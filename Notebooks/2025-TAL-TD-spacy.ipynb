{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master HN PSL ‚Äì TAL 08/10/2025\n",
    "\n",
    "## [Spacy](https://spacy.io)\n",
    "\n",
    "Notebook con√ßu par C. Plancq, revu et mis √† jour par T. Poibeau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Biblioth√®que logicielle de TAL √©crite en Python (et Cython)\n",
    "- √âtiquetage POS, lemmatisation, analyse syntaxique, entit√©s nomm√©es, word embedding, transformers\n",
    "- Usage de mod√®les neuronaux\n",
    "- Int√©gration ais√©e de biblioth√®ques de deep learning\n",
    "- v3.0.3 ([github](https://github.com/explosion/spaCy))\n",
    "- Licence MIT (Open Source) pour le code\n",
    "    - Licences ouvertes diverses pour les mod√®les\n",
    "- Produit de la soci√©t√© [explosion.ai](https://explosion.ai/). Fond√© par :¬†Matthew Honnibal ([@honnibal](https://twitter.com/honnibal)) et Ines Montani ([@_inesmontani](https://twitter.com/_inesmontani))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi Spacy ?\n",
    "\n",
    "- C'est du Python üôå üéâ\n",
    "- Plut√¥t simple √† prendre en main\n",
    "- Tr√®s bien document√©, √† notre avis. D'ailleurs plut√¥t que ce notebook, suivez l'excellent tutorial d'Ines Montani : [https://course.spacy.io/](https://course.spacy.io/)\n",
    "- Couvre les traitements d'une cha√Æne de TAL typique\n",
    "- Pas mal utilis√© dans l'industrie\n",
    "- MAIS ce n'est pas forc√©ment l'outil qui donne les meilleurs r√©sultats pour le fran√ßais dans toutes les t√¢ches de TAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy et les autres\n",
    "\n",
    "Spacy est *un* des frameworks de TAL disponibles\n",
    "\n",
    "- [NLTK](http://www.nltk.org/) :¬†python, orient√© p√©dagogie, pas de mod√®les neuronaux inclus mais se combine bien avec TensorFlow, PyTorch ou AlleNLP\n",
    "- [Stanford Core¬†NLP](https://stanfordnlp.github.io/stanfordnlp/) :¬†java, mod√®les pour 53 langues (UD), r√©solution de la cor√©ference.\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/) :¬†python, nouveau framework de Stanford, mod√®les neuronaux entra√Æn√©s sur donn√©es UD <small>[https://github.com/explosion/spacy-stanza](https://github.com/explosion/spacy-stanza) permet d'utiliser les mod√®les de Stanford avec Spacy</small>\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "- [DKPro](https://dkpro.github.io/)\n",
    "- [flair](https://github.com/zalandoresearch/flair) : le framework de Zalando, tr√®s bonnes performances en reconnaissance d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation\n",
    "\n",
    "dans un terminal\n",
    "```bash\n",
    "python3 -m pip install -U --user spacy \n",
    "#ou pip install -U --user spacy\n",
    "```\n",
    "- installation du mod√®le fran√ßais\n",
    "```bash\n",
    "python3 -m spacy download fr_core_news_md\n",
    "#ou python3 -m spacy download fr_core_news_sm \n",
    "```\n",
    "- v√©rification\n",
    "```bash\n",
    "python3 -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mod√®les\n",
    "\n",
    "- Ce qui suit n'est pas forc√©ment √† jour. Voir la documentation Spacy officielle pour des informations plus r√©centes. \n",
    "\n",
    "- Spacy utilise des mod√®les statistiques qui permettent de pr√©dire des annotations linguistiques\n",
    "- 16 langues :¬†allemand, anglais, chinois, danois, espagnol, fran√ßais, italien, japonais, lituanien, n√©erlandais, grec, norv√©gien, polonais, portugais, roumain, russe + mod√®le multi langues\n",
    "- 4 mod√®les pour le fran√ßais\n",
    "    - fr_core_news_sm (tagger, morphologizer, lemmatizer, parser, ner) 16 Mo\n",
    "    - fr_core_news_md (tagger, morphologizer, lemmatizer, parser, ner, vectors) 45 Mo\n",
    "    - fr_core_news_lg (tagger, morphologizer, lemmatizer, parser, ner, vectors) 546 Mo\n",
    "    - fr_dep_news_trf (tagger, morphologizer, lemmatizer, parser) 381 Mo\n",
    "- mod√®les `fr` appris sur les corpus [Sequoia](https://deep-sequoia.inria.fr/fr/) et [WikiNer](https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500) sauf le mod√®le `trf` qui est issu de camembert-base distribu√© par [Hugging Face](https://huggingface.co/camembert-base).\n",
    "- Tous ces mod√®les, quelque soient leur type ou leur langue, s'utilisent de la m√™me fa√ßon, avec la m√™me API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    "- *si vous voulez utiliser Spacy prenez le temps de lire la [documentation](https://spacy.io/usage), ici ce ne sera qu'un coup d'≈ìil incomplet*\n",
    "- un mod√®le est une instance de la classe `Language`, il est adapt√© √† une langue en particulier\n",
    "- un mod√®le incorpore un vocabulaire, des poids, des vecteurs de mots, une configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- le traitement fonctionne avec un [*pipeline*](https://spacy.io/usage/spacy-101#pipelines) pour convertir un texte en objet `Doc` (texte annot√©)\n",
    "- par d√©faut `tokenizer` > `tagger` > `parser` > `ner` > `‚Ä¶`\n",
    "- depuis la v3 le pipeline devient `tok2vec` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`  \n",
    "  ou `transformer` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`\n",
    "- l'utilisateur peut ajouter des √©tapes ou en retrancher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md', disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour au pipeline par d√©faut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Un objet `Doc` est une s√©quence d'objets `Token` (voir l'[API](https://spacy.io/api/token))\n",
    " - Le texte d'origine est d√©coup√© en phrases, tokeniz√©, annot√© en POS, lemme, syntaxe (d√©pendance) et en entit√©s nomm√©es (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"L‚ÄôOrganisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì tokenization\n",
    "\n",
    "La tokenization de Spacy est non-destructive. Vous pouvez d√©couper un texte en tokens et le restituer dans sa forme originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text_with_ws, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì √©tiquetage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les annotations portant sur les tokens sont accessibles via les attributs des objets de type `token`‚ÄØ: [https://spacy.io/api/token#attributes](https://spacy.io/api/token#attributes)  \n",
    "  - `pos_` contient l'√©tiquette de partie du discours de [universal dependancies](https://universaldependencies.org/docs/u/pos/)\n",
    "  - `tag_` contient l'√©tiquette du corpus original, parfois plus d√©taill√©e\n",
    "  - `lemma_` pour le lemme\n",
    "  - `morph` pour l'analyse morphologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.morph, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traiter plusieurs textes en s√©rie, il est recommand√© d'utiliser [nlp.pipe](https://spacy.io/api/language#pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Cadine avait un tr√®s-mauvais caract√®re. Elle ne s‚Äôaccommodait pas du r√¥le de servante.\",\n",
    "    \"Aussi finit-elle par s‚Äô√©tablir pour son compte.\",\n",
    "    \"Comme elle √©tait alors √¢g√©e de treize ans, et qu‚Äôelle ne pouvait r√™ver le grand commerce, un banc de vente de l‚Äôall√©e aux fleurs, elle vendit des bouquets de violettes d‚Äôun sou, piqu√©s dans un lit de mousse, sur un √©ventaire d‚Äôosier pendu √† son cou.\",\n",
    "    \"Elle r√¥dait toute la journ√©e dans les Halles, autour des Halles, promenant son bout de pelouse.\",\n",
    "    \"C‚Äô√©tait l√† sa joie, cette fl√¢nerie continuelle, qui lui d√©gourdissait les jambes, qui la tirait des longues heures pass√©es √† faire des bouquets, les genoux pli√©s, sur une chaise basse.\",\n",
    "    \"Maintenant, elle tournait ses violettes en marchant, elle les tournait comme des fuseaux, avec une merveilleuse l√©g√®ret√© de doigts ; elle comptait six √† huit fleurs, selon la saison, pliait en deux un brin de jonc, ajoutait une feuille, roulait un fil mouill√© ; et, entre ses dents de jeune loup, elle cassait le fil.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "1. Extrayez de la s√©rie de phrases ci-dessus la liste des noms communs\n",
    "2. Comptez le nombre de tokens au masculin et au f√©minin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si NER (*Named Entity Recognition*) fait partie de votre mod√®le, vos donn√©es seront annot√©es √©galement en entit√©s nomm√©es.  \n",
    "Vous pouvez y acc√©der avec l'attribut `ent_type_` des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou acc√©der directement aux entit√©s de l'objet `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy int√®gre un outil de visualisation pour l'annotation en entit√©s nomm√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Le pr√©sident Xi Jinping a affirm√© que la propagation du coronavirus √©tait ¬´‚ÄØpratiquement jugul√©e‚ÄØ¬ª. Il s‚Äôest d‚Äôailleurs rendu pour la premi√®re fois √† Wuhan, la capitale de la province du Hubei, le berceau du Covid-19.')\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "\n",
    "Dans `data/Le_Ventre_de_Paris-short.txt` (ou un texte de votre choix), comptez la fr√©quence de chaque entit√© de type PER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì analyse syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse syntaxique ou *parsing* de Spacy est une analyse en d√©pendance. La plupart sinon la totalit√© des mod√®les utilis√©s viennent de https://universaldependencies.org\n",
    "\n",
    "Dans l'analyse en d√©pendance produite par Spacy, chaque mot d'une phrase a un gouverneur unique (*head*), la relation de d√©pendance entre le mot et son gouverneur est typ√©e (*nsubj*, *obj*, ‚Ä¶).  \n",
    "Pour la t√™te de la phrase on utilise la relation *ROOT*.\n",
    "\n",
    "La structure produite par l'analyse syntaxique est un arbre, un graphe acyclique et connexe. Les tokens sont les n≈ìuds, les arcs sont les d√©pendances, le type de la relation est l'√©tiquette de l'arc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`displacy` fournit un outil de visualisation bien pratique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe √©galement un outil issu d'un d√©veloppement ind√©pendant :¬†[explacy](https://spacy.io/universe/project/explacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp, 'Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi r√©cup√©rer parcourir les tokens et afficher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_.upper(), token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les attributs de token suivant peuvent √™tre utilis√©s pour parcourir l'arbre de d√©pendance :¬†\n",
    "- `children` les tokens d√©pendants du token\n",
    "- `subtree` tous les descendants du token\n",
    "- `ancestors` tous les parents du token\n",
    "- `rights` les enfants √† droite du token\n",
    "- `lefts` les enfants √† gauche du token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire de la phrase pr√©c√©dente le triplet sujet-verbe-objet comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = [token for token in doc if token.head == token][0]\n",
    "subjects = [tok for tok in root.lefts if tok.dep_ == \"nsubj\"]\n",
    "subject = subjects[0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ == \"obj\"]\n",
    "obj = objs[0]\n",
    "subject, root, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous\n",
    "\n",
    "1. Trouver et afficher l'objet de la phrase :¬†¬´ Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle. ¬ª\n",
    "\n",
    "2. Que remarque-t-on ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching par r√®gle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy a une classe `Matcher` qui permet de rep√©rer des tokens ou des suites de tokens √† l'aide de patrons (*pattern*). Ces patrons peuvent porter sur la forme des tokens ou leurs attributs (pos, ent).  \n",
    "On peut aussi utiliser des cat√©gories comme `IS_ALPHA` ou `IS_NUM`, voir la [doc](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "# en taille + lettres en maj\n",
    "matcher.add(\"tailles\", [pattern])\n",
    "\n",
    "doc = nlp(\"Ce mod√®le est aussi disponible en taille M ; je vous le conseille.\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√áa fonctionne pour les s√©quences comme ¬´ en taille M ¬ª ou ¬´ en taille XL ¬ª mais pas pour ¬´ vous l'avez en XL ? ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut essayer d'am√©liorer les r√®gles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou encore :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "sizes = ['XS', 'S', 'M', 'L', 'XL']\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Dans `data/Le_Ventre_de_Paris-short.txt`, trouver les s√©quences pronom - le lemme 'vendre'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependancy Matcher :¬†extraction de patrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis la v3, Spacy a ajout√© un *Dependancy Matcher* qui permet de faire de l'extraction de patrons syntaxiques. Il est maintenant possible de faire porter des requ√™tes sur l'arbre syntaxique et non plus seulement sur la s√©quence des tokens.  \n",
    "Ce dispositif utilise [Semgrex](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html), la syntaxe utilis√©e dans Tgrep et Tregex, les outils de requ√™te sur Treebank de Stanford.\n",
    "\n",
    "Voir la [documentation](https://spacy.io/usage/rule-based-matching#dependencymatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventre_short = \"\"\n",
    "with open('data/Le_Ventre_de_Paris-short.txt') as input_f:\n",
    "    ventre_short = input_f.read()\n",
    "doc = nlp(ventre_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"vendre\",    \n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"vendre\"}\n",
    "  }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    for t_id in t_ids:\n",
    "        print(doc[t_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"vendre\",    \n",
    "        \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"vendre\", \"acheter\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"sujet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},  \n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"objet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obj\", \"iobj\", \"obl\"]}},  \n",
    "    }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    print(\"verbe, sujet, objet :¬†\", \" -> \".join([doc[t_id].text for t_id in t_ids]))\n",
    "    print(\"objet complet :¬†\", \" \".join([t.text for t in doc[t_ids[2]].subtree]))\n",
    "    print(\"Phrase compl√©te :¬†\", doc[t_ids[0]].sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Ajouter une r√®gle au motif pour trouver aussi l'objet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter les traitements de Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. re-tokenisation\n",
    "\n",
    "- voir [https://spacy.io/usage/linguistic-features#retokenization](https://spacy.io/usage/linguistic-features#retokenization)\n",
    "\n",
    "Dans l'exemple qui suit ¬´ quer-cra ¬ª sera tokeniz√© √† tort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_)for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[7:], attrs={\"LEMMA\": \"quer-cra\", \"POS\": \"NOUN\"})\n",
    "print([(tok.text, tok.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ici c‚Äôest l‚Äôobjet doc qui est modifi√©, le r√©sultat mais pas le traitement. Nous allons voir comment faire pour modifier le traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modification de la tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "special_case = [{ORTH: \"quer-cra\"}]\n",
    "nlp.tokenizer.add_special_case(\"quer-cra\", special_case)\n",
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien modifi√© la tokenisation dans le mod√®le `nlp`. Cela n'affecte pas par contre l'√©tiquetage en POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entit√©s nomm√©es :¬†traitement par r√®gles\n",
    " - Voir [https://spacy.io/usage/rule-based-matching#entityruler](https://spacy.io/usage/rule-based-matching#entityruler)\n",
    " \n",
    "Spacy offre aussi un m√©canisme de traitement par r√®gle pour les entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(\"Depuis que Machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Avant : \", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={'overwrite_ents':True})\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Chrome\"},\n",
    "            {\"label\":\"ORG\", \"pattern\":\"Machin\"},\n",
    "    {\"label\":\"ORG\", \"pattern\":\"Criteo\"},\n",
    "    {\"label\":\"ORG\",\"pattern\":\"LiveRamp\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Depuis que Machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Apr√®s : \", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
